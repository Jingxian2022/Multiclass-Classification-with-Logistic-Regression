{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e952d5dc-4aac-4597-87a0-f5236107613a",
   "metadata": {},
   "source": [
    "# **DATA 2060 Final Project**\n",
    "\n",
    "### Written by: Rui Gao,\n",
    "### Link to the github repo: https://github.com/Jingxian2022/Multiclass-Classification-with-Logistic-Regression/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e50bc8",
   "metadata": {},
   "source": [
    "## **Overview of Multiclass Classification with Logistic Regression**\n",
    "\n",
    "### Algorithm Overview\n",
    "\n",
    "Multiclass classification with logistic regression extends the standard logistic regression approach, which traditionally handles binary classification, to address problems involving more than two classes. \n",
    "\n",
    "### Multiclass Classification Basics\n",
    "- Problem Definition  \n",
    "Multiclass classification is the problem of classifying instances into one of three or more classes.\n",
    "\n",
    "- Input and Output \n",
    "  - Inputs $X$ typically come from a feature space.\n",
    "  - Outputs $Y$ are from a finite set of labels $ Y = \\{1, 2, \\ldots, k\\} $, where $ k $ is the number of classes.\n",
    "\n",
    "### Multiclass Classification Strategies\n",
    "In binary logistic regression, a linear function predicts the probability of the positive class using a logistic (sigmoid) function. Extending this to multiclass classification can be done using the following approaches:\n",
    "\n",
    "##### One-vs-All Approach\n",
    "One-vs-All involves training a single binary classifier for each class, with the samples of that class as positive samples and all other samples as negatives. The class with the highest probability score is selected for each input.\n",
    "\n",
    "##### All-pairs Approach\n",
    "All-pairs involves training $\\binom{k}{2} = k(k - 1)/2$ binary classifiers, each receives the samples of a pair of classes from the original training set, and learn to distinguish these two classes. For prediction, all $k (k − 1) / 2$ classifiers are applied to an unseen sample and the class that got the highest number of \"+1\" predictions gets predicted by the combined classifier.\n",
    "\n",
    "### Logistic Regression\n",
    "Logistic Regression is a statistical method used for binary classification, predicting one of two possible outcomes based on input features. It estimates the parameters of a logistic model (the coefficients in the linear or non linear combinations) and transforms the linear combination of features using the sigmoid function, which maps any real-valued number into a value between 0 and 1. Logistic regression belongs to the family of generalized linear models and is widely used when the target variable is binary. \n",
    "\n",
    "#### Loss Function\n",
    "In logistic regression, the loss function quantifies the error between the predicted probabilities and the actual class labels. The most commonly used loss function for binary logistic regression is logistic loss(sometimes called cross-entropy loss). This function aims to minimize the log loss across all training observations. By penalizing incorrect predictions, the loss function encourages the model to produce probabilities that are closer to the true class labels.\n",
    "\n",
    "#### Optimization\n",
    "Gradient descent and its variants, like stochastic gradient descent (SGD), are common optimization techniques for logistic regression. Gradient descent works by computing the gradient (partial derivatives) of the loss function with respect to each parameter, and updating each parameter in the opposite direction of the gradient to minimize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619710c4",
   "metadata": {},
   "source": [
    "\n",
    "### Representation\n",
    "\n",
    "#### Logistic regression\n",
    "Logistic regression is common hypothesis class for classification\n",
    "\n",
    "$$ \\mathcal{X} = \\mathbb{R}^d \\quad \\mathcal{Y} = \\{1, -1\\} $$\n",
    "\n",
    "Now we use a linear predictor that outputs a continuous value in [0, 1]\n",
    "\n",
    "$$ h_w(\\mathbf{x}) = \\frac{1}{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x} \\rangle}} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\mathbf{x} \\in \\mathcal{X}$ represents the input vector with dimension $d$\n",
    "* $\\mathbf{w}$ is the weight vector\n",
    "* $\\langle \\mathbf{w}, \\mathbf{x} \\rangle$ denotes the dot product between $\\mathbf{w}$ and $\\mathbf{x}$\n",
    "\n",
    "This linear predictor maps to:\n",
    "\n",
    "$$ h : \\mathcal{X} \\rightarrow [0, 1] $$\n",
    "\n",
    "#### One-versus-All Pseudo Code\n",
    "input:  \n",
    "* training set $S = (x_1, y_1), \\ldots, (x_m, y_m)$\n",
    "* algorithm for binary classification $ A $ (here $A$ is Logistic Regression)\n",
    "\n",
    "foreach $ i \\in \\mathcal{Y} $:   \n",
    "* let $ S_i = (x_1, (-1)^{\\mathbb{I}_{[y_1 \\neq i]}}), \\ldots, (x_m, (-1)^{\\mathbb{I}_{[y_m \\neq i]}}) $\n",
    "* let $ h_i = A(S_i) $\n",
    "\n",
    "output:  \n",
    "- the multiclass hypothesis defined by $ h(x) \\in \\arg\\max_{i \\in \\mathcal{Y}} h_i(x) $\n",
    "\n",
    "#### All-Pairs Pseudo Code\n",
    "input:  \n",
    "- training set $ S = (x_1, y_1), \\ldots, (x_m, y_m) $\n",
    "- algorithm for binary classification $ A $ (here $A$ is Logistic Regression)\n",
    "\n",
    "foreach $ i, j \\in \\mathcal{Y} $ such that $ i < j $:\n",
    "- initialize $ S_{i, j} $ to be the empty sequence\n",
    "- for $ t = 1, \\ldots, m $:\n",
    "  - If $ y_t = i $, add $ (x_t, 1) $ to $ S_{i, j} $\n",
    "  - If $ y_t = j $, add $ (x_t, -1) $ to $ S_{i, j} $\n",
    "- let $ h_{i, j} = A(S_{i, j}) $\n",
    "\n",
    "output:  \n",
    "- the multiclass hypothesis defined by  \n",
    "  $ h(x) \\in \\arg\\max_{i \\in \\mathcal{Y}} \\left( \\sum_{j \\in \\mathcal{Y}} \\text{sign}(j - i) h_{i, j}(x) \\right) $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef43d70c-38e7-47b7-a3ed-08c59dfa1103",
   "metadata": {},
   "source": [
    "\n",
    "### Loss\n",
    "\n",
    "For binary classification, logistic regression uses the sigmoid function:\n",
    "$$P(y = 1 | x) = \\sigma(w^{T}x + b)$$\n",
    "Where:\n",
    "* $x$ is the input vector\n",
    "* $w$ is the weights\n",
    "* $b$ is the bias\n",
    "* $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid function\n",
    "\n",
    "Binary Cross-Entropy Loss:\n",
    "$$L(y, \\hat y) = -(y log(\\hat y) + (1 - y)log(1 - \\hat y))$$\n",
    "Where:\n",
    "* $y$ is the true label (0 or 1)\n",
    "* $\\hat y$ is the predicted probability of the first class\n",
    "* and $\\hat y = \\sigma(w^T x + b)$\n",
    "\n",
    "One-vs-All:\n",
    "For one-vs-all, we have to train $K$ different classifiers for each class so that each classifier $k$ can learn to distinguish one class from all the others.\n",
    "The loss for the $i$-th example of classifier $k$ is:\n",
    "$$L_k(y^{(i)}, \\hat y_k^{(i)}) = -[y_k^{(i)}log(\\hat y_k^{(i)}) + (1 - y_k^{(i)})log(1 - \\hat y_k^{(i)})]$$\n",
    "Where:\n",
    "* $y_k^{(i)} = 1$ if the true class of the $i$-th example is class $k$, otherwise $y_k^{(i)} = 0$\n",
    "* $\\hat y_k^{(i)}$ is the predicted probability for class $k$\n",
    "\n",
    "The overall class is determined by selecting the classifier that has the highest probability (or confidence).\n",
    "\n",
    "All-Pairs:\n",
    "For All-Pairs, we have to train a classifier for every pair of classes instead of $K$ classifiers in One-vs-All training. For $K$ classes, we train $\\frac{K(K - 1)}{2}$ classifiers to distinguish between 2 classes for each classifier.\n",
    "\n",
    "The loss function is still the binary cross-entropy loss and rewritten for the $i$-th example as:\n",
    "$$L_{k,j}(y_{k,j}^{(i)}, \\hat y_{k, j}^{(i)}) = -[y_{k, j}^{(i)}log(\\hat y_{k, j}^{(i)}) + (1 - y_{k, j}^{(i)})log(1 - \\hat y_{k, j}^{(i)})]$$\n",
    "Each classifier will vote for one of two classes and the overall class is the class that receives the most votes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff72865c",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "### One-vs-All (OvR) with SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ff686",
   "metadata": {},
   "source": [
    "In One-vs-All, we train a separate binary classifier for each class. Each classifier learns to distinguish one class from all others. Below is the pesudo code on sample S."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a69fb",
   "metadata": {},
   "source": [
    "$\\text{Initialize parameters } \\mathbf{w} \\text{ for each class, learning rate } \\alpha, \\text{ and batch size } b$<br />\n",
    "$\\text{converge} = \\text{False}$<br />\n",
    "\n",
    "$\\text{while not converge:}$ <br />\n",
    "    $\\quad \\text{epoch} += 1$<br />\n",
    "    $\\quad \\text{Shuffle training examples}$<br />\n",
    "    $\\quad \\text{Calculate last epoch loss}$<br />\n",
    "    \n",
    "$\\quad \\text{for } i = 0, 1, \\dots, \\left\\lceil \\frac{n_{\\text{examples}}}{b} \\right\\rceil - 1 \\text{: } \\quad \\text{(iterate over batches)}$<br />\n",
    "        $\\quad \\quad X_{\\text{batch}} = X[i \\cdot b : (i + 1) \\cdot b] \\quad \\text{(select the } X \\text{ in the current batch)}$<br />\n",
    "        $\\quad \\quad \\mathbf{y}_{\\text{batch}} = \\mathbf{y}[i \\cdot b : (i + 1) \\cdot b] \\quad \\text{(select the labels in the current batch)}$<br />\n",
    "        $\\quad \\quad \\nabla L_{\\mathbf{w}} = \\mathbf{0} \\quad \\text{(initialize gradient matrix for each class)}$\n",
    "\n",
    "$\\quad \\quad \\text{for each pair of training data } (x, y) \\in (X_{\\text{batch}}, \\mathbf{y}_{\\text{batch}}) \\text{:}$<br />\n",
    "            $\\quad \\quad \\quad \\text{for } j = 0, 1, \\dots, n_{\\text{classes}} - 1 \\text{:}$<br />\n",
    "                $\\quad \\quad \\quad \\quad \\text{if } y = j \\text{:}$<br />\n",
    "                    $\\quad \\quad \\quad \\quad \\quad \\nabla L_{\\mathbf{w}_j} += \\left( \\sigma(\\mathbf{w}_j^T x) - 1\\right) \\cdot x  \\quad \\text{(for correct class)}$<br />\n",
    "                    $\\quad \\quad \\quad \\quad \\text{else:}$<br />\n",
    "                    $\\quad \\quad \\quad \\quad \\quad \\nabla L_{\\mathbf{w}_j} += \\sigma(\\mathbf{w}_j^T x) \\cdot x  \\quad \\text{(for other classes)}$<br />\n",
    "\n",
    "$\\quad \\quad \\text{for } j = 0, 1, \\dots, n_{\\text{classes}} - 1 \\text{:}$<br />\n",
    "            $\\quad \\quad \\quad \\mathbf{w}_j = \\mathbf{w}_j - \\alpha \\cdot \\frac{\\nabla L_{\\mathbf{w}_j}}{\\text{len}(X_{\\text{batch}})}  \\quad \\text{(update weights for each class)}$<br />\n",
    "\n",
    "$\\quad \\text{Calculate this epoch loss}$<br />\n",
    "    $\\quad \\text{if } \\left| \\text{Loss}(X, \\mathbf{y})_{\\text{this-epoch}} - \\text{Loss}(X, \\mathbf{y})_{\\text{last-epoch}} \\right| < \\text{CONV-THRESHOLD:}$<br />\n",
    "        $\\quad \\quad \\text{converge} = \\text{True}  \\quad \\text{(break the loop if loss converged)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b86973d",
   "metadata": {},
   "source": [
    "Here, $sigmoid(w_j^T x)$ gives the probability that \n",
    "𝑥\n",
    "x belongs to class \n",
    "𝑗\n",
    " (treated as a binary classification for that specific class)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d8e248",
   "metadata": {},
   "source": [
    "### All Pairs (OvO) with SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f650d12b",
   "metadata": {},
   "source": [
    "In All Pairs, we train a separate binary classifier for each pair of classes, focusing only on the data points belonging to the two classes in each pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafdcb87",
   "metadata": {},
   "source": [
    "$\\text{Initialize parameters } \\mathbf{w} \\text{ for each pair of classes, learning rate } \\alpha, \\text{ and batch size } b$<br />\n",
    "$\\text{converge} = \\text{False}$<br />\n",
    "\n",
    "$\\text{while not converge:}$<br />\n",
    "    $\\quad \\text{epoch} += 1$<br />\n",
    "    $\\quad \\text{Shuffle training examples}$<br />\n",
    "    $\\quad \\text{Calculate last epoch loss}$<br />\n",
    "    \n",
    "$\\quad \\text{for } i = 0, 1, \\dots, \\left\\lceil \\frac{n_{\\text{examples}}}{b} \\right\\rceil - 1 \\text{: } \\quad \\text{(iterate over batches)}$<br />\n",
    "        $\\quad \\quad X_{\\text{batch}} = X[i \\cdot b : (i + 1) \\cdot b] \\quad \\text{(select the } X \\text{ in the current batch)}$<br />\n",
    "        $\\quad \\quad \\mathbf{y}_{\\text{batch}} = \\mathbf{y}[i \\cdot b : (i + 1) \\cdot b] \\quad \\text{(select the labels in the current batch)}$<br />\n",
    "        $\\quad \\quad \\text{for each unique pair of classes } (A, B) \\text{:}$<br />\n",
    "            $\\quad \\quad \\quad \\nabla L_{\\mathbf{w}_{AB}} = \\mathbf{0} \\quad \\text{(initialize gradient for each pair (A, B))}$<br />\n",
    "            $\\quad \\quad \\quad \\text{for each } (x, y) \\in (X_{\\text{batch}}, \\mathbf{y}_{\\text{batch}}) \\text{:}$<br />\n",
    "                $\\quad \\quad \\quad \\quad \\text{if } y = A \\text{ or } y = B \\text{:} \\quad \\text{(focus on examples for classes A and B)}$<br />\n",
    "                    $\\quad \\quad \\quad \\quad \\quad \\text{if } y = A \\text{:}$<br />\n",
    "                        $\\quad \\quad \\quad \\quad \\quad \\quad \\nabla L_{\\mathbf{w}_{AB}} += \\left( \\sigma(\\mathbf{w}_{AB}^T x) - 1 \\right) \\cdot x  \\quad \\text{(for class A)}$<br />\n",
    "                    $\\quad \\quad \\quad \\quad \\text{else:}$<br />\n",
    "                        $\\quad \\quad \\quad \\quad \\quad \\quad \\nabla L_{\\mathbf{w}_{AB}} += \\sigma(\\mathbf{w}_{AB}^T x) \\cdot x  \\quad \\text{(for class B)}$<br />\n",
    "\n",
    "$\\quad \\quad \\quad \\mathbf{w}_{AB} = \\mathbf{w}_{AB} - \\alpha \\cdot \\frac{\\nabla L_{\\mathbf{w}_{AB}}}{\\text{len}(X_{\\text{batch}})}  \\quad \\text{(update weights for the pair (A, B))}$<br />\n",
    "\n",
    "$\\quad \\text{Calculate this epoch loss}$<br />\n",
    "    $\\quad \\text{if } \\left| \\text{Loss}(X, \\mathbf{y})_{\\text{this-epoch}} - \\text{Loss}(X, \\mathbf{y})_{\\text{last-epoch}} \\right| < \\text{CONV-THRESHOLD:}$<br />\n",
    "        $\\quad \\quad \\text{converge} = \\text{True}  \\quad \\text{(break the loop if loss converged)}$\n",
    "\n",
    "            \n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6279dc14-6d01-4746-aad6-cd813f603f3e",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e93a7c8-3421-4a36-aea1-afc139438ec9",
   "metadata": {},
   "source": [
    "https://github.com/danhergir/Logistic_regression </br>\n",
    "https://danhergir.medium.com/implementing-multi-class-logistic-regression-with-scikit-learn-53d919b72c13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece999df-cced-4094-9ed7-3559e77299a8",
   "metadata": {},
   "source": [
    "Run the environment test below and make sure all the requirements are met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b7718d4b-422a-4966-8055-418cf7f098ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.12.5\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.9.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 2.0.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.5.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 2.2.2 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.12.5 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.12.5\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.12.5\"):\n",
    "    print(FAIL, \"Python version 3.12.5 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'matplotlib': \"3.9.1\", 'numpy': \"2.0.1\",'sklearn': \"1.5.1\", \n",
    "                'pandas': \"2.2.2\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cee7a1",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc33c40-0f67-435d-9b4f-a2f021901a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    '''\n",
    "    Apply softmax to an array.\n",
    "    @params:\n",
    "        x: The original array.\n",
    "    @return:\n",
    "        An array with softmax applied elementwise.\n",
    "    '''\n",
    "    e = np.exp(x - np.max(x))\n",
    "    return (e + 1e-6) / (np.sum(e) + 1e-6)\n",
    "\n",
    "class MulticlassLogisticRegression:\n",
    "    '''\n",
    "    Multiclass Logistic Regression with One-vs-All (OvA) and All-Pairs (OvO) strategies,\n",
    "    trained using stochastic gradient descent.\n",
    "    '''\n",
    "    def __init__(self, n_features, n_classes, batch_size=32, conv_threshold=1e-4, strategy='one-vs-all'):\n",
    "        '''\n",
    "        Initializes the Multiclass Logistic Regression classifier.\n",
    "        @attrs:\n",
    "            n_features: Number of features in the dataset.\n",
    "            n_classes: Number of unique classes.\n",
    "            weights: Model weights, initialized to zeros.\n",
    "            strategy: Multiclass strategy ('one-vs-all' or 'all-pairs').\n",
    "            alpha: Learning rate for SGD.\n",
    "        '''\n",
    "        self.n_classes = n_classes\n",
    "        self.n_features = n_features\n",
    "        self.strategy = strategy\n",
    "        self.weights = None  # Initialize dynamically based on the strategy\n",
    "        self.alpha = 0.03  \n",
    "        self.batch_size = batch_size\n",
    "        self.conv_threshold = conv_threshold\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        '''\n",
    "        Trains the model using stochastic gradient descent.\n",
    "        Supports both One-vs-All and All-Pairs strategies.\n",
    "        @params:\n",
    "            X: 2D Numpy array where each row is an example, padded with one column for bias.\n",
    "            Y: 1D Numpy array of labels for each example.\n",
    "        @return:\n",
    "            Number of epochs taken to converge.\n",
    "        '''\n",
    "        if self.strategy == 'one-vs-all':\n",
    "            self._train_one_vs_all(X, Y)\n",
    "        elif self.strategy == 'all-pairs':\n",
    "            self._train_all_pairs(X, Y)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid strategy: {self.strategy}. Use 'one-vs-all' or 'all-pairs'.\")\n",
    "\n",
    "    def _train_one_vs_all(self, X, Y):\n",
    "        '''\n",
    "        Trains the model using the One-vs-All (OvA) strategy. \n",
    "        Each class is treated as a binary classification problem against all other classes, \n",
    "        and a separate weight vector is trained for each class.\n",
    "\n",
    "        @params:\n",
    "            X: A 2D Numpy array where each row is a feature vector of an example, \n",
    "               padded with one column for the bias term.\n",
    "            Y: A 1D Numpy array of class labels for each example in X.\n",
    "            \n",
    "            Labels are converted into binary format for each class during training.\n",
    "        '''\n",
    "        self.weights = np.zeros((self.n_classes, self.n_features + 1))\n",
    "        for class_label in range(self.n_classes):\n",
    "            binary_Y = (Y == class_label).astype(int)\n",
    "            self._train_binary_class(X, binary_Y, class_label)\n",
    "\n",
    "    def _train_all_pairs(self, X, Y):\n",
    "        '''\n",
    "        Trains the model using the All-Pairs (OvO) strategy.\n",
    "        Each pair of classes is treated as a binary classification problem, \n",
    "        and a separate weight vector is trained for each class pair.\n",
    "\n",
    "        @params:\n",
    "            X: A 2D Numpy array where each row is a feature vector of an example, \n",
    "               padded with one column for the bias term.\n",
    "            Y: A 1D Numpy array of class labels for each example in X.\n",
    "            \n",
    "            Only examples belonging to any two distinct classes are used for training each classifier.\n",
    "        '''\n",
    "        self.weights = {}\n",
    "        for i in range(self.n_classes):\n",
    "            for j in range(i + 1, self.n_classes):\n",
    "                indices = np.where((Y == i) | (Y == j))[0]\n",
    "                X_subset = X[indices]\n",
    "                Y_subset = Y[indices]\n",
    "                binary_Y = (Y_subset == i).astype(int)\n",
    "                self.weights[(i, j)] = np.zeros(self.n_features + 1)\n",
    "                self._train_binary_class(X_subset, binary_Y, (i, j))\n",
    "\n",
    "    def _train_binary_class(self, X, Y, label):\n",
    "        '''\n",
    "        Trains a binary logistic regression model for a specific class or pair of classes.\n",
    "        @params:\n",
    "            X: A 2D Numpy array where each row contains a feature vector for a training example.\n",
    "            Y: A 1D Numpy array with binary labels (0 or 1) corresponding to the examples in X.\n",
    "            label: An integer (for OvA) or tuple (for OvO) representing the class or class pair being trained.\n",
    "        @return:\n",
    "            Number of epochs taken to converge during the training process.\n",
    "        '''\n",
    "        num_examples = X.shape[0]\n",
    "        epoch = 0\n",
    "        converged = False\n",
    "        last_loss = float('inf')\n",
    "        while not converged:\n",
    "            epoch += 1\n",
    "            indices = np.arange(num_examples)\n",
    "            np.random.shuffle(indices)\n",
    "            X = X[indices]\n",
    "            Y = Y[indices]\n",
    "            grad_w = np.zeros_like(self.weights[label] if isinstance(label, tuple) else self.weights[label])\n",
    "\n",
    "            for i in range(num_examples // self.batch_size):\n",
    "                batch_X = X[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                batch_Y = Y[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "\n",
    "                for x, y in zip(batch_X, batch_Y):\n",
    "                    raw = grad_w @ x\n",
    "                    prob = softmax(raw)\n",
    "                    grad_w += (prob - y) * x\n",
    "\n",
    "            grad_w /= num_examples\n",
    "            self.weights[label] -= self.alpha * grad_w\n",
    "            this_loss = self.loss(X, Y, label)\n",
    "            if abs(this_loss - last_loss) < self.conv_threshold:\n",
    "                converged = True\n",
    "            last_loss = this_loss\n",
    "\n",
    "        return epoch\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predicts the class for each example in X.\n",
    "        @params:\n",
    "            X: 2D Numpy array of examples, padded with one column for bias.\n",
    "        @return:\n",
    "            1D Numpy array of predicted class labels.\n",
    "        '''\n",
    "        if self.strategy == 'one-vs-all':\n",
    "            return self._predict_one_vs_all(X)\n",
    "        elif self.strategy == 'all-pairs':\n",
    "            return self._predict_all_pairs(X)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid strategy: {self.strategy}. Use 'one-vs-all' or 'all-pairs'.\")\n",
    "\n",
    "    def _predict_one_vs_all(self, X):\n",
    "        '''\n",
    "        Predicts the class labels for a given dataset using the One-vs-All (OvA) strategy.\n",
    "        @params:\n",
    "            X: A 2D Numpy array where each row is a feature vector of an example, padded with one column for the bias term.\n",
    "        @return:\n",
    "            A 1D Numpy array containing the predicted class labels for each example in X.\n",
    "            Each label corresponds to the class with the highest probability.\n",
    "        '''\n",
    "        probabilities = np.dot(X, self.weights.T)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "    def _predict_all_pairs(self, X):\n",
    "        '''\n",
    "        Predicts the class labels for a given dataset using the All-Pairs (OvO) strategy.\n",
    "        @params:\n",
    "            X: A 2D Numpy array where each row is an example, padded with one column for bias.\n",
    "        @return:\n",
    "            A 1D Numpy array of predicted class labels for each example in X.\n",
    "        '''\n",
    "        votes = np.zeros((X.shape[0], self.n_classes))\n",
    "        for (i, j), weight in self.weights.items():\n",
    "            raw = X @ weight\n",
    "            predictions = (raw >= 0).astype(int)\n",
    "            votes[:, i] += predictions\n",
    "            votes[:, j] += (1 - predictions)\n",
    "        return np.argmax(votes, axis=1)\n",
    "\n",
    "    def loss(self, X, Y, label=None):\n",
    "        '''\n",
    "        Computes the log loss for the model.\n",
    "        @params:\n",
    "            X: 2D Numpy array of examples, padded with one column for bias.\n",
    "            Y: 1D Numpy array of labels for each example.\n",
    "            label: Binary classification label or class pair.\n",
    "        @return:\n",
    "            Average log loss.\n",
    "        '''\n",
    "        total_loss = 0\n",
    "        num_examples = X.shape[0]\n",
    "        for x, y in zip(X, Y):\n",
    "            raw = self.weights[label] @ x if label else self.weights @ x\n",
    "            prob = softmax(raw)\n",
    "            total_loss += -np.log(prob[y] + 1e-6)\n",
    "        return total_loss / num_examples\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        '''\n",
    "        Computes accuracy on a given dataset.\n",
    "        @params:\n",
    "            X: 2D Numpy array of examples, padded with one column for bias.\n",
    "            Y: 1D Numpy array of true labels.\n",
    "        @return:\n",
    "            Float value representing accuracy.\n",
    "        '''\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bd21b4-d52e-4cd2-93fe-f7be11099657",
   "metadata": {},
   "source": [
    "### Check Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cce6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary classification test model weight:  [[ 0.          0.          0.        ]\n",
      " [ 0.32823945 -0.35773025 -0.02791037]]\n",
      "binary classification test model loss:  0.23235306890809349\n",
      "binary classification test model accuracy:  0.8\n",
      "sklearn model weight:  [[ 1.67860606 -1.60222429 -0.14913757]]\n",
      "sklearn model accuracy:  0.8\n",
      "One-vs-all test model weights:  [[-1.3724442  -1.32429544  1.93852442]\n",
      " [-0.9301881   1.34266162 -2.00148668]\n",
      " [ 1.21446654 -1.16280427 -1.22034655]]\n",
      "One-vs-all test model accuracy:  1.0\n",
      "sklearn ova model accuracy:  0.75\n",
      "sklearn ova model weight: \n",
      "[[ -7.89594806 -10.49554171  22.17456811]]\n",
      "[[ -4.24504487   8.4105686  -16.85444343]]\n",
      "[[ 4.52013308 -1.52390628 -8.01910973]]\n",
      "All-pairs test model weights:  {(0, 1): array([ 0.        , -3.89957518,  7.35592563]), (0, 2): array([-2.80449105,  1.53512679,  4.59244867]), (1, 2): array([-0.97042227,  1.01034898,  0.11759221])}\n",
      "All-pairs test model accuracy:  1.0\n",
      "sklearn ovo model accuracy:  1.0\n",
      "[[ -7.89594806 -10.49554171  22.17456811]]\n",
      "[[ -4.24504487   8.4105686  -16.85444343]]\n",
      "[[ 4.52013308 -1.52390628 -8.01910973]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pytest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "x_bias = np.array([[0,4,1], [0,3,1], [5,0,1], [4,1,1], [0,5,1]])\n",
    "x = x_bias[:,:-1]\n",
    "y = np.array([0,0,1,1,0])\n",
    "x_bias_test = np.array([[0,0,1], [-5,3,1], [9,0,1], [1,0,1], [6,-7,1]])\n",
    "x_test = x_bias_test[:,:-1]\n",
    "y_test = np.array([0,0,1,0,1])\n",
    "\n",
    "# test binary classification model\n",
    "binary_test_model = MulticlassLogisticRegression(2, 2, 5, 1e-2)\n",
    "binary_test_model.weights = np.zeros((2, 3))\n",
    "binary_test_model._train_binary_class(x_bias, y, label=1)\n",
    "print(\"binary classification test model weight: \",binary_test_model.weights)\n",
    "print(\"binary classification test model loss: \", binary_test_model.loss(x_bias, y, 1))\n",
    "print(\"binary classification test model accuracy: \", binary_test_model.accuracy(x_bias_test, y_test))\n",
    "\n",
    "# sklearn LogisticRegression\n",
    "lg_model = LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000, tol=1e-2)\n",
    "lg_model.fit(x, y)\n",
    "print(\"sklearn model weight: \", np.hstack([lg_model.coef_, lg_model.intercept_.reshape(-1, 1)]))\n",
    "print(\"sklearn model accuracy: \", lg_model.score(x_test,y_test))\n",
    "# y_proba = lg_model.predict_log_proba(x)\n",
    "# print(y_proba)\n",
    "# print(\"sklearn model logloss :\",log_loss(y, y_proba))\n",
    "\n",
    "x_bias2 = np.array([[0,0,1], [0,3,1], [4,0,1], [6,1,1], [0,1,1], [0,4,1]])\n",
    "y2 = np.array([0,1,2,2,0,1])\n",
    "x_bias_test2 = np.array([[0,0,1], [-5,3,1], [9,0,1], [1,0,1]])\n",
    "y_test2 = np.array([0,1,2,0])\n",
    "x2 = x_bias2[:,:-1]\n",
    "x2_test = x_bias_test2[:,:-1]\n",
    "\n",
    "# test the multiclass classification with one-vs-all\n",
    "test_model_one_vs_all = MulticlassLogisticRegression(2, 3)\n",
    "test_model_one_vs_all.train(x_bias2, y2)\n",
    "print(\"One-vs-all test model weights: \", test_model_one_vs_all.weights)\n",
    "print(\"One-vs-all test model accuracy: \", test_model_one_vs_all.accuracy(x_bias_test2,y_test2))\n",
    "\n",
    "# sklearn multiclass classification with one-vs-all\n",
    "logistic_regression_model = LogisticRegression(penalty=None, solver='lbfgs', max_iter=10000, tol=1e-4)\n",
    "ova_model = OneVsRestClassifier(logistic_regression_model)\n",
    "ova_model.fit(x2, y2)\n",
    "print(\"sklearn ova model weight: \")\n",
    "for i, estimator in enumerate(ova_model.estimators_):\n",
    "    weights = estimator.coef_\n",
    "    bias = estimator.intercept_\n",
    "    weights_with_bias = np.hstack([weights, bias.reshape(-1, 1)])\n",
    "    print(weights_with_bias)\n",
    "print(\"sklearn ova model accuracy: \", ova_model.score(x2_test,y_test2))\n",
    "\n",
    "# test the multiclass classification with all-pairs\n",
    "test_model_all_pairs = MulticlassLogisticRegression(2,3,strategy=\"all-pairs\")\n",
    "test_model_all_pairs.train(x_bias2, y2)\n",
    "print(\"All-pairs test model weights: \", test_model_all_pairs.weights)\n",
    "print(\"All-pairs test model accuracy: \", test_model_all_pairs.accuracy(x_bias_test2,y_test2))\n",
    "\n",
    "# sklearn multiclass classification with all-pairs\n",
    "ovo_model = OneVsOneClassifier(logistic_regression_model)\n",
    "ovo_model.fit(x2, y2)\n",
    "print(\"sklearn ovo model weight: \")\n",
    "for i, estimator in enumerate(ova_model.estimators_):\n",
    "    weights = estimator.coef_\n",
    "    bias = estimator.intercept_\n",
    "    weights_with_bias = np.hstack([weights, bias.reshape(-1, 1)])\n",
    "    print(weights_with_bias)\n",
    "print(\"sklearn ovo model accuracy: \", ovo_model.score(x2_test,y_test2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b2a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p2/g87nqpyx22vf3yf9_6dx3hwc0000gn/T/ipykernel_56417/3127999900.py:31: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_over.replace(list(np.unique(y_over)), [1, 2, 3, 4, 5, 6, 7], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-vs-all model accuracy:  0.7578659370725034\n",
      "Library model accuracy:  0.9863201094391245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p2/g87nqpyx22vf3yf9_6dx3hwc0000gn/T/ipykernel_56417/3127999900.py:31: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_over.replace(list(np.unique(y_over)), [1, 2, 3, 4, 5, 6, 7], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All-pairs model accuracy:  0.7510259917920656\n",
      "Library model accuracy:  0.9931600547195623\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "DATA_FILE = '../data/Dry_Bean.csv'\n",
    "\n",
    "def get_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Class'].unique()\n",
    "\n",
    "    undersample = RandomUnderSampler(random_state=42)\n",
    "\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df.Class\n",
    "\n",
    "    X_over, y_over = undersample.fit_resample(X, y)\n",
    "\n",
    "    # sns.countplot(x=y_over, data=df)\n",
    "    # plt.xticks(rotation=45)\n",
    "    # plt.show()\n",
    "\n",
    "    y_over.replace(list(np.unique(y_over)), [1, 2, 3, 4, 5, 6, 7], inplace=True)\n",
    "    df_dea = X_over\n",
    "    df_dea['Class'] = y_over\n",
    "    \n",
    "    # This columns may create an overfitted model\n",
    "    X_over.drop(['ConvexArea', 'EquivDiameter'], axis=1, inplace=True)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, random_state=0, shuffle=True, test_size=.2)\n",
    "    \n",
    "    # scale our data\n",
    "    st_x = StandardScaler()\n",
    "    X_train = st_x.fit_transform(X_train)\n",
    "    X_test = st_x.transform(X_test)\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_test = y_test.to_numpy()\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def test_dry_bean_ovr():\n",
    "    X_train, Y_train, X_test, Y_test = get_data(DATA_FILE)\n",
    "    num_features = X_train.shape[1]\n",
    "    NUM_CLASS = 7\n",
    "    BATCH_SIZE = 100\n",
    "    CONV_THRESHOLD = 1e-3\n",
    "\n",
    "    X_train_b = np.hstack((X_train, np.ones((X_train.shape[0], 1))))\n",
    "    X_test_b = np.hstack((X_test, np.ones((X_test.shape[0], 1))))\n",
    "\n",
    "    model = MulticlassLogisticRegression(num_features, NUM_CLASS, BATCH_SIZE, CONV_THRESHOLD)\n",
    "    model.train(X_train_b, Y_train)\n",
    "    acc = model.accuracy(X_test_b, Y_test)\n",
    "    print(\"One-vs-all model accuracy: \",acc)\n",
    "\n",
    "    logistic_regression_model = LogisticRegression(solver='liblinear')\n",
    "    ova_model = OneVsRestClassifier(logistic_regression_model)\n",
    "    ova_model.fit(X_train, Y_train)\n",
    "    print(\"Library model accuracy: \",ova_model.score(X_test,Y_test))\n",
    "    \n",
    "def test_dry_bean_ovo():\n",
    "    X_train, Y_train, X_test, Y_test = get_data(DATA_FILE)\n",
    "    num_features = X_train.shape[1]\n",
    "    NUM_CLASS = 7\n",
    "    BATCH_SIZE = 100\n",
    "    CONV_THRESHOLD = 1e-3\n",
    "    \n",
    "    X_train_b = np.hstack((X_train, np.ones((X_train.shape[0], 1))))\n",
    "    X_test_b = np.hstack((X_test, np.ones((X_test.shape[0], 1))))\n",
    "\n",
    "    model = MulticlassLogisticRegression(num_features, NUM_CLASS, BATCH_SIZE, CONV_THRESHOLD, 'all-pairs')\n",
    "    model.train(X_train_b, Y_train)\n",
    "    acc = model.accuracy(X_test_b, Y_test)\n",
    "    print(\"All-pairs model accuracy: \",acc)\n",
    "\n",
    "    logistic_regression_model = LogisticRegression(solver='liblinear')\n",
    "    ovo_model = OneVsOneClassifier(logistic_regression_model)\n",
    "    ovo_model.fit(X_train, Y_train)\n",
    "    print(\"Library model accuracy: \",ovo_model.score(X_test,Y_test))\n",
    "\n",
    "test_dry_bean_ovr()\n",
    "test_dry_bean_ovo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
