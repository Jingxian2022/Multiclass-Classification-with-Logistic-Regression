{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e952d5dc-4aac-4597-87a0-f5236107613a",
   "metadata": {},
   "source": [
    "# **DATA 2060 Final Project**\n",
    "\n",
    "### Written by: Allison Gao,\n",
    "### Link to the github repo: https://github.com/Jingxian2022/Multiclass-Classification-with-Logistic-Regression/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e50bc8",
   "metadata": {},
   "source": [
    "## **Overview of Multiclass Classification with Logistic Regression**\n",
    "\n",
    "### Algorithm Overview\n",
    "\n",
    "Multiclass classification with logistic regression extends the standard logistic regression approach, which traditionally handles binary classification, to address problems involving more than two classes. \n",
    "\n",
    "### Multiclass Classification Basics\n",
    "- Problem Definition  \n",
    "Multiclass classification is the problem of classifying instances into one of three or more classes.\n",
    "\n",
    "- Input and Output \n",
    "  - Inputs $X$ typically come from a feature space.\n",
    "  - Outputs $Y$ are from a finite set of labels $ Y = \\{1, 2, \\ldots, k\\} $, where $ k $ is the number of classes.\n",
    "\n",
    "### Multiclass Classification Strategies\n",
    "In binary logistic regression, a linear function predicts the probability of the positive class using a logistic (sigmoid) function. Extending this to multiclass classification can be done using the following approaches:\n",
    "\n",
    "##### One-vs-All Approach\n",
    "One-vs-All involves training a single binary classifier for each class, with the samples of that class as positive samples and all other samples as negatives. The class with the highest probability score is selected for each input.\n",
    "\n",
    "##### All-pairs Approach\n",
    "All-pairs involves training $\\binom{k}{2} = k(k - 1)/2$ binary classifiers, each receives the samples of a pair of classes from the original training set, and learn to distinguish these two classes. For prediction, all $k (k − 1) / 2$ classifiers are applied to an unseen sample and the class that got the highest number of \"+1\" predictions gets predicted by the combined classifier.\n",
    "\n",
    "### Logistic Regression\n",
    "Logistic Regression is a statistical method used for binary classification, predicting one of two possible outcomes based on input features. It estimates the parameters of a logistic model (the coefficients in the linear or non linear combinations) and transforms the linear combination of features using the sigmoid function, which maps any real-valued number into a value between 0 and 1. Logistic regression belongs to the family of generalized linear models and is widely used when the target variable is binary. \n",
    "\n",
    "#### Loss Function\n",
    "In logistic regression, the loss function quantifies the error between the predicted probabilities and the actual class labels. The most commonly used loss function for binary logistic regression is logistic loss(sometimes called cross-entropy loss). This function aims to minimize the log loss across all training observations. By penalizing incorrect predictions, the loss function encourages the model to produce probabilities that are closer to the true class labels.\n",
    "\n",
    "#### Optimization\n",
    "Gradient descent and its variants, like stochastic gradient descent (SGD), are common optimization techniques for logistic regression. Gradient descent works by computing the gradient (partial derivatives) of the loss function with respect to each parameter, and updating each parameter in the opposite direction of the gradient to minimize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619710c4",
   "metadata": {},
   "source": [
    "\n",
    "### Representation\n",
    "\n",
    "#### Logistic regression\n",
    "Logistic regression is common hypothesis class for classification\n",
    "\n",
    "$$ \\mathcal{X} = \\mathbb{R}^d \\quad \\mathcal{Y} = \\{1, -1\\} $$\n",
    "\n",
    "Now we use a linear predictor that outputs a continuous value in [0, 1]\n",
    "\n",
    "$$ h_w(\\mathbf{x}) = \\frac{1}{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x} \\rangle}} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\mathbf{x} \\in \\mathcal{X}$ represents the input vector with dimension $d$\n",
    "* $\\mathbf{w}$ is the weight vector\n",
    "* $\\langle \\mathbf{w}, \\mathbf{x} \\rangle$ denotes the dot product between $\\mathbf{w}$ and $\\mathbf{x}$\n",
    "\n",
    "This linear predictor maps to:\n",
    "\n",
    "$$ h : \\mathcal{X} \\rightarrow [0, 1] $$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef43d70c-38e7-47b7-a3ed-08c59dfa1103",
   "metadata": {},
   "source": [
    "\n",
    "### Loss\n",
    "\n",
    "For binary classification, logistic regression uses the sigmoid function:\n",
    "$$P(y = 1 | x) = \\sigma(w^{T}x + b)$$\n",
    "Where:\n",
    "* $x$ is the input vector\n",
    "* $w$ is the weights\n",
    "* $b$ is the bias\n",
    "* $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid function\n",
    "\n",
    "Binary Cross-Entropy Loss:\n",
    "$$L(y, \\hat y) = -(y log(\\hat y) + (1 - y)log(1 - \\hat y))$$\n",
    "Where:\n",
    "* $y$ is the true label (0 or 1)\n",
    "* $\\hat y$ is the predicted probability of the first class\n",
    "* and $\\hat y = \\sigma(w^T x + b)$\n",
    "\n",
    "One-vs-All:\n",
    "For one-vs-all, we have to train $K$ different classifiers for each class so that each classifier $k$ can learn to distinguish one class from all the others.\n",
    "The loss for the $i$-th example of classifier $k$ is:\n",
    "$$L_k(y^{(i)}, \\hat y_k^{(i)}) = -[y_k^{(i)}log(\\hat y_k^{(i)}) + (1 - y_k^{(i)})log(1 - \\hat y_k^{(i)})]$$\n",
    "Where:\n",
    "* $y_k^{(i)} = 1$ if the true class of the $i$-th example is class $k$, otherwise $y_k^{(i)} = 0$\n",
    "* $\\hat y_k^{(i)}$ is the predicted probability for class $k$\n",
    "\n",
    "The overall class is determined by selecting the classifier that has the highest probability (or confidence).\n",
    "\n",
    "All-Pairs:\n",
    "For All-Pairs, we have to train a classifier for every pair of classes instead of $K$ classifiers in One-vs-All training. For $K$ classes, we train $\\frac{K(K - 1)}{2}$ classifiers to distinguish between 2 classes for each classifier.\n",
    "\n",
    "The loss function is still the binary cross-entropy loss and rewritten for the $i$-th example as:\n",
    "$$L_{k,j}(y_{k,j}^{(i)}, \\hat y_{k, j}^{(i)}) = -[y_{k, j}^{(i)}log(\\hat y_{k, j}^{(i)}) + (1 - y_{k, j}^{(i)})log(1 - \\hat y_{k, j}^{(i)})]$$\n",
    "Each classifier will vote for one of two classes and the overall class is the class that receives the most votes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff72865c",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "The optimizer used in this implementation is Stochastic Gradient Descent (SGD). SGD is an iterative optimization algorithm that updates the model parameters (weights and biases) by minimizing the loss function, specifically the cross-entropy loss for multiclass classification problems. Below are the psudo-codes for finding the optimizer for the one-vs-all and the all-pairs algorithms with SGD.\n",
    "### One-vs-All (OvR) with SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ff686",
   "metadata": {},
   "source": [
    "In One-vs-All, we train a separate binary classifier for each class. Each classifier learns to distinguish one class from all others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a69fb",
   "metadata": {},
   "source": [
    "$\\text{Initialize parameters } \\mathbf{w} \\text{ for each class, learning rate } \\alpha, \\text{ and batch size } b$<br />\n",
    "$\\text{converge} = \\text{False}$<br />\n",
    "\n",
    "$\\text{while not converge:}$ <br />\n",
    "    $\\quad \\text{epoch} += 1$<br />\n",
    "    $\\quad \\text{Shuffle training examples}$<br />\n",
    "    \n",
    "$\\quad \\text{for } i = 0, 1, \\dots, \\left\\lceil \\frac{n_{\\text{examples}}}{b} \\right\\rceil - 1 \\text{: } \\quad \\text{(iterate over batches)}$<br />\n",
    "        $\\quad \\quad X_{\\text{batch}} = X[i \\cdot b : (i + 1) \\cdot b] \\quad \\text{(select the } X \\text{ in the current batch)}$<br />\n",
    "        $\\quad \\quad \\mathbf{y}_{\\text{batch}} = \\mathbf{y}[i \\cdot b : (i + 1) \\cdot b] \\quad \\text{(select the labels in the current batch)}$<br />\n",
    "        $\\quad \\quad \\nabla L_{\\mathbf{w}} = \\mathbf{0} \\quad \\text{(initialize gradient matrix for each class)}$\n",
    "\n",
    "$\\quad \\quad \\text{for each pair of training data } (x, y) \\in (X_{\\text{batch}}, \\mathbf{y}_{\\text{batch}}) \\text{:}$<br />\n",
    "            $\\quad \\quad \\quad \\text{for } j = 0, 1, \\dots, n_{\\text{classes}} - 1 \\text{:}$<br />\n",
    "                $\\quad \\quad \\quad \\quad \\text{if } y = j \\text{:}$<br />\n",
    "                    $\\quad \\quad \\quad \\quad \\quad \\nabla L_{\\mathbf{w}_j} += \\left( \\sigma(\\mathbf{w}_j^T x) - 1\\right) \\cdot x  \\quad \\text{(for correct class, reflects how much the predicted probability deviates from the true label (1).)}$<br />\n",
    "                    $\\quad \\quad \\quad \\quad \\text{else:}$<br />\n",
    "                    $\\quad \\quad \\quad \\quad \\quad \\nabla L_{\\mathbf{w}_j} += \\sigma(\\mathbf{w}_j^T x) \\cdot x  \\quad \\text{(for other classes)}$<br />\n",
    "$\\quad \\quad \\quad \\mathbf{w}_j = \\mathbf{w}_j - \\alpha \\cdot \\frac{\\nabla L_{\\mathbf{w}_j}}{\\text{len}(X_{\\text{batch}})}  \\quad \\text{(update weights for each class)}$<br />\n",
    "\n",
    "$\\quad \\text{Calculate this epoch loss}$<br />\n",
    "    $\\quad \\text{if } \\left| \\text{Loss}(X, \\mathbf{y})_{\\text{this-epoch}} - \\text{Loss}(X, \\mathbf{y})_{\\text{last-epoch}} \\right| < \\text{CONV-THRESHOLD:}$<br />\n",
    "        $\\quad \\quad \\text{converge} = \\text{True}  \\quad \\text{(break the loop if loss converged)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b86973d",
   "metadata": {},
   "source": [
    "Here, $\\sigma(w_j^T x)$ gives the probability that \n",
    "𝑥 belongs to class 𝑗 (treated as a binary classification for that specific class)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d8e248",
   "metadata": {},
   "source": [
    "### All Pairs (OvO) with SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f650d12b",
   "metadata": {},
   "source": [
    "In All Pairs, we train a separate binary classifier for each pair of classes, focusing only on the data points belonging to the two classes in each pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafdcb87",
   "metadata": {},
   "source": [
    "$\\text{Initialize parameters } \\mathbf{w} \\text{ for each pair of classes, learning rate } \\alpha, \\text{ and batch size } b$<br />\n",
    "$\\text{converge} = \\text{False}$<br />\n",
    "\n",
    "$\\text{while not converge:}$<br />\n",
    "    $\\quad \\text{epoch} += 1$<br />\n",
    "    $\\quad \\text{Shuffle training examples}$<br />\n",
    "    \n",
    "$\\quad \\text{for } i = 0, 1, \\dots, \\left\\lceil \\frac{n_{\\text{examples}}}{b} \\right\\rceil - 1 \\text{: } \\quad \\text{(iterate over batches)}$<br />\n",
    "        $\\quad \\quad X_{\\text{batch}} = X[i \\cdot b : (i + 1) \\cdot b] \\quad \\text{(select the } X \\text{ in the current batch)}$<br />\n",
    "        $\\quad \\quad \\mathbf{y}_{\\text{batch}} = \\mathbf{y}[i \\cdot b : (i + 1) \\cdot b] \\quad \\text{(select the labels in the current batch)}$<br />\n",
    "        $\\quad \\quad \\text{for each unique pair of classes } (A, B) \\text{:}$<br />\n",
    "            $\\quad \\quad \\quad \\nabla L_{\\mathbf{w}_{AB}} = \\mathbf{0} \\quad \\text{(initialize gradient for each pair (A, B))}$<br />\n",
    "            $\\quad \\quad \\quad \\text{for each } (x, y) \\in (X_{\\text{batch}}, \\mathbf{y}_{\\text{batch}}) \\text{:}$<br />\n",
    "                $\\quad \\quad \\quad \\quad \\text{if } y = A \\text{ or } y = B \\text{:} \\quad \\text{(focus on examples for classes A and B)}$<br />\n",
    "                    $\\quad \\quad \\quad \\quad \\quad \\text{if } y = A \\text{:}$<br />\n",
    "                        $\\quad \\quad \\quad \\quad \\quad \\quad \\nabla L_{\\mathbf{w}_{AB}} += \\left( \\sigma(\\mathbf{w}_{AB}^T x) - 1 \\right) \\cdot x  \\quad \\text{(for class A)}$<br />\n",
    "                    $\\quad \\quad \\quad \\quad \\text{else:}$<br />\n",
    "                        $\\quad \\quad \\quad \\quad \\quad \\quad \\nabla L_{\\mathbf{w}_{AB}} += \\sigma(\\mathbf{w}_{AB}^T x) \\cdot x  \\quad \\text{(for class B)}$<br />\n",
    "\n",
    "$\\quad \\quad \\quad \\mathbf{w}_{AB} = \\mathbf{w}_{AB} - \\alpha \\cdot \\frac{\\nabla L_{\\mathbf{w}_{AB}}}{\\text{len}(X_{\\text{batch}})}  \\quad \\text{(update weights for the pair (A, B))}$<br />\n",
    "\n",
    "$\\quad \\text{Calculate this epoch loss}$<br />\n",
    "    $\\quad \\text{if } \\left| \\text{Loss}(X, \\mathbf{y})_{\\text{this-epoch}} - \\text{Loss}(X, \\mathbf{y})_{\\text{last-epoch}} \\right| < \\text{CONV-THRESHOLD:}$<br />\n",
    "        $\\quad \\quad \\text{converge} = \\text{True}  \\quad \\text{(break the loop if loss converged)}$\n",
    "\n",
    "            \n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6279dc14-6d01-4746-aad6-cd813f603f3e",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e93a7c8-3421-4a36-aea1-afc139438ec9",
   "metadata": {},
   "source": [
    "https://github.com/danhergir/Logistic_regression </br>\n",
    "https://danhergir.medium.com/implementing-multi-class-logistic-regression-with-scikit-learn-53d919b72c13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece999df-cced-4094-9ed7-3559e77299a8",
   "metadata": {},
   "source": [
    "Run the environment test below and make sure all the requirements are met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7718d4b-422a-4966-8055-418cf7f098ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.12.7\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.9.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 2.0.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.5.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 2.2.2 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.12.5 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.12.5\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.12.5\"):\n",
    "    print(FAIL, \"Python version 3.12.5 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'matplotlib': \"3.9.1\", 'numpy': \"2.0.1\",'sklearn': \"1.5.1\", \n",
    "                'pandas': \"2.2.2\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cee7a1",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bc33c40-0f67-435d-9b4f-a2f021901a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''\n",
    "    Apply sigmoid function to an array.\n",
    "    @params:\n",
    "        x: The input array.\n",
    "    @return:\n",
    "        An array with the sigmoid function applied elementwise.\n",
    "    '''\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class MulticlassLogisticRegression:\n",
    "    '''\n",
    "    Multiclass Logistic Regression with One-vs-All (OvA) and All-Pairs (OvO) strategies,\n",
    "    trained using stochastic gradient descent.\n",
    "    '''\n",
    "    def __init__(self, n_features, n_classes, batch_size=32, conv_threshold=1e-4, strategy='one-vs-all'):\n",
    "        '''\n",
    "        Initializes the Multiclass Logistic Regression classifier.\n",
    "        @attrs:\n",
    "            n_features: Number of features in the dataset.\n",
    "            n_classes: Number of unique classes.\n",
    "            weights: Model weights, initialized to zeros.\n",
    "            strategy: Multiclass strategy ('one-vs-all' or 'all-pairs').\n",
    "            alpha: Learning rate for SGD.\n",
    "        '''\n",
    "        self.n_classes = n_classes\n",
    "        self.n_features = n_features\n",
    "        self.strategy = strategy\n",
    "        self.weights = None  # Initialize dynamically based on the strategy\n",
    "        self.alpha = 0.03  \n",
    "        self.batch_size = batch_size\n",
    "        self.conv_threshold = conv_threshold\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        '''\n",
    "        Trains the model using stochastic gradient descent.\n",
    "        Supports both One-vs-All and All-Pairs strategies.\n",
    "        @params:\n",
    "            X: 2D Numpy array where each row is an example, padded with one column for bias.\n",
    "            Y: 1D Numpy array of labels for each example.\n",
    "        @return:\n",
    "            Number of epochs taken to converge.\n",
    "        '''\n",
    "        if self.strategy == 'one-vs-all':\n",
    "            self._train_one_vs_all(X, Y)\n",
    "        elif self.strategy == 'all-pairs':\n",
    "            self._train_all_pairs(X, Y)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid strategy: {self.strategy}. Use 'one-vs-all' or 'all-pairs'.\")\n",
    "\n",
    "    def _train_one_vs_all(self, X, Y):\n",
    "        '''\n",
    "        Trains the model using the One-vs-All (OvA) strategy. \n",
    "        Each class is treated as a binary classification problem against all other classes, \n",
    "        and a separate weight vector is trained for each class.\n",
    "\n",
    "        @params:\n",
    "            X: A 2D Numpy array where each row is a feature vector of an example, \n",
    "               padded with one column for the bias term.\n",
    "            Y: A 1D Numpy array of class labels for each example in X.\n",
    "            \n",
    "            Labels are converted into binary format for each class during training.\n",
    "        '''\n",
    "        self.weights = np.zeros((self.n_classes, self.n_features + 1))\n",
    "        for class_label in range(self.n_classes):\n",
    "            binary_Y = (Y == class_label).astype(int) #if label matches then assign 1, otherwise 0\n",
    "            self._train_binary_class(X, binary_Y, class_label)\n",
    "\n",
    "    def _train_all_pairs(self, X, Y):\n",
    "        '''\n",
    "        Trains the model using the All-Pairs (OvO) strategy.\n",
    "        Each pair of classes is treated as a binary classification problem, \n",
    "        and a separate weight vector is trained for each class pair.\n",
    "\n",
    "        @params:\n",
    "            X: A 2D Numpy array where each row is a feature vector of an example, \n",
    "               padded with one column for the bias term.\n",
    "            Y: A 1D Numpy array of class labels for each example in X.\n",
    "            \n",
    "            Only examples belonging to any two distinct classes are used for training each classifier.\n",
    "        '''\n",
    "        #The weights for all binary classifiers are stored in a dictionary\n",
    "        #Keys: Tuples representing a pair of classes (e.g., (0, 1), (0, 2))\n",
    "        #Values: Weight vectors for the corresponding classifier.\n",
    "        self.weights = {}\n",
    "        \n",
    "        #a total of n(n-1)/2 classifiers are trained\n",
    "        for i in range(self.n_classes):\n",
    "            for j in range(i + 1, self.n_classes):\n",
    "                #identifies the indices of examples where the label is either i or j\n",
    "                indices = np.where((Y == i) | (Y == j))[0]\n",
    "                X_subset = X[indices]\n",
    "                Y_subset = Y[indices]\n",
    "\n",
    "                #labels converted into binary format\n",
    "                binary_Y = (Y_subset == i).astype(int) #class i = 1, class j = 0\n",
    "                self.weights[(i, j)] = np.zeros(self.n_features + 1)\n",
    "                self._train_binary_class(X_subset, binary_Y, (i, j))\n",
    "\n",
    "    def _train_binary_class(self, X, Y, label):\n",
    "        '''\n",
    "        Trains a binary logistic regression model for a specific class or pair of classes.\n",
    "        @params:\n",
    "            X: A 2D Numpy array where each row contains a feature vector for a training example.\n",
    "            Y: A 1D Numpy array with binary labels (0 or 1) corresponding to the examples in X.\n",
    "            label: An integer (for OvA) or tuple (for OvO) representing the class or class pair being trained.\n",
    "        @return:\n",
    "            Number of epochs taken to converge during the training process.\n",
    "        '''\n",
    "        num_examples = X.shape[0]\n",
    "        epoch = 0\n",
    "        converged = False\n",
    "        last_loss = float('inf')\n",
    "        while not converged:\n",
    "            epoch += 1\n",
    "            indices = np.arange(num_examples)\n",
    "            np.random.shuffle(indices)\n",
    "            X = X[indices]\n",
    "            Y = Y[indices]\n",
    "                \n",
    "            for i in range(int(np.ceil(num_examples/self.batch_size))):\n",
    "                batch_X = X[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                batch_Y = Y[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "\n",
    "                grad_w = np.zeros_like(self.weights[label] if isinstance(label, tuple) else self.weights[label])\n",
    "                for x, y in zip(batch_X, batch_Y):\n",
    "                    raw = np.dot(self.weights[label], x)\n",
    "                    prob = sigmoid(raw)  # Probability of positive class\n",
    "                    grad_w += (prob - y) * x\n",
    "\n",
    "                grad_w /= len(batch_X)\n",
    "                self.weights[label] -= self.alpha * grad_w\n",
    "                \n",
    "            this_loss = self.loss(X, Y, label)\n",
    "            if abs(this_loss - last_loss) < self.conv_threshold:\n",
    "                converged = True\n",
    "                \n",
    "            last_loss = this_loss\n",
    "\n",
    "        return epoch\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predicts the class for each example in X.\n",
    "        @params:\n",
    "            X: 2D Numpy array of examples, padded with one column for bias.\n",
    "        @return:\n",
    "            1D Numpy array of predicted class labels.\n",
    "        '''\n",
    "        if self.strategy == 'one-vs-all':\n",
    "            return self._predict_one_vs_all(X)\n",
    "        elif self.strategy == 'all-pairs':\n",
    "            return self._predict_all_pairs(X)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid strategy: {self.strategy}. Use 'one-vs-all' or 'all-pairs'.\")\n",
    "\n",
    "    def _predict_one_vs_all(self, X):\n",
    "        '''\n",
    "        Predicts the class labels for a given dataset using the One-vs-All (OvA) strategy.\n",
    "        @params:\n",
    "            X: A 2D Numpy array where each row is a feature vector of an example, padded with one column for the bias term.\n",
    "        @return:\n",
    "            A 1D Numpy array containing the predicted class labels for each example in X.\n",
    "            Each label corresponds to the class with the highest probability.\n",
    "        '''\n",
    "        probabilities = np.dot(X, self.weights.T)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "    def _predict_all_pairs(self, X):\n",
    "        '''\n",
    "        Predicts the class labels for a given dataset using the All-Pairs (OvO) strategy.\n",
    "        @params:\n",
    "            X: A 2D Numpy array where each row is an example, padded with one column for bias.\n",
    "        @return:\n",
    "            A 1D Numpy array of predicted class labels for each example in X.\n",
    "        '''\n",
    "        votes = np.zeros((X.shape[0], self.n_classes))\n",
    "        for (i, j), weight in self.weights.items():\n",
    "            #raw score for the (i,j) classifier\n",
    "            raw = X @ weight\n",
    "            #1 or class i if >= 0, 0 or class j if < 0, decision boundary\n",
    "            predictions = (raw >= 0).astype(int)\n",
    "            votes[:, i] += predictions\n",
    "            votes[:, j] += (1 - predictions)\n",
    "        #select class with the most votes\n",
    "        return np.argmax(votes, axis=1)\n",
    "\n",
    "    def loss(self, X, Y, label):\n",
    "        '''\n",
    "        Computes the log loss for the model.\n",
    "        @params:\n",
    "            X: 2D Numpy array of examples, padded with one column for bias.\n",
    "            Y: 1D Numpy array of labels for each example.\n",
    "            label: Binary classification label or class pair.\n",
    "        @return:\n",
    "            Average log loss.\n",
    "        '''\n",
    "        total_loss = 0\n",
    "        num_examples = X.shape[0]\n",
    "\n",
    "        if isinstance(label, tuple):\n",
    "            # Binary classification loss (OvO for a specific class pair)\n",
    "            for x, y in zip(X, Y):\n",
    "                raw = np.dot(self.weights[label], x)  # Raw score for the OvO classifier\n",
    "                prob = sigmoid(raw)  # Sigmoid for binary probabilities\n",
    "                if y == label[0]:  # Positive class in the pair\n",
    "                    total_loss += -np.log(prob + 1e-6)\n",
    "                else:  # Negative class in the pair\n",
    "                    total_loss += -np.log(1 - prob + 1e-6)\n",
    "        else:\n",
    "            # Binary classification loss (OvA for a specific class)\n",
    "            for x, y in zip(X, Y):\n",
    "                raw = np.dot(self.weights[label], x)  # Raw score for the OvA classifier\n",
    "                probability = sigmoid(raw)  # Sigmoid for binary probabilities\n",
    "                if y == 1:  # Positive class\n",
    "                    total_loss += -np.log(probability + 1e-6)\n",
    "                else:  # Negative class (all other classes)\n",
    "                    total_loss += -np.log(1 - probability + 1e-6)\n",
    "\n",
    "        return total_loss / num_examples\n",
    "   \n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        '''\n",
    "        Computes accuracy on a given dataset.\n",
    "        @params:\n",
    "            X: 2D Numpy array of examples, padded with one column for bias.\n",
    "            Y: 1D Numpy array of true labels.\n",
    "        @return:\n",
    "            Float value representing accuracy.\n",
    "        '''\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd21b4-d52e-4cd2-93fe-f7be11099657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
