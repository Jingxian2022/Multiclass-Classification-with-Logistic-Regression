{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564d99a-8aff-424f-9e54-6b9018fec7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Overview of Multiclass Classification with Logistic Regression**\n",
    "\n",
    "### Algorithm Overview\n",
    "\n",
    "Multiclass classification with logistic regression extends the standard logistic regression approach, which traditionally handles binary classification, to address problems involving more than two classes. \n",
    "\n",
    "### Multiclass Classification Basics\n",
    "- Problem Definition  \n",
    "Multiclass classification is the problem of classifying instances into one of three or more classes.\n",
    "\n",
    "- Input and Output \n",
    "  - Inputs $X$ typically come from a feature space.\n",
    "  - Outputs $Y$ are from a finite set of labels $ Y = \\{1, 2, \\ldots, k\\} $, where $ k $ is the number of classes.\n",
    "\n",
    "### Multiclass Classification Strategies\n",
    "In binary logistic regression, a linear function predicts the probability of the positive class using a logistic (sigmoid) function. Extending this to multiclass classification can be done using the following approaches:\n",
    "\n",
    "##### One-vs-All Approach\n",
    "One-vs-All involves training a single binary classifier for each class, with the samples of that class as positive samples and all other samples as negatives. The class with the highest probability score is selected for each input.\n",
    "\n",
    "##### All-pairs Approach\n",
    "All-pairs involves training $\\binom{k}{2} = k(k - 1)/2$ binary classifiers, each receives the samples of a pair of classes from the original training set, and learn to distinguish these two classes. For prediction, all $k (k ‚àí 1) / 2$ classifiers are applied to an unseen sample and the class that got the highest number of \"+1\" predictions gets predicted by the combined classifier.\n",
    "\n",
    "### Logistic Regression\n",
    "Logistic Regression is a statistical method used for binary classification, predicting one of two possible outcomes based on input features. It estimates the parameters of a logistic model (the coefficients in the linear or non linear combinations) and transforms the linear combination of features using the sigmoid function, which maps any real-valued number into a value between 0 and 1. Logistic regression belongs to the family of generalized linear models and is widely used when the target variable is binary. \n",
    "\n",
    "#### Loss Function\n",
    "In logistic regression, the loss function quantifies the error between the predicted probabilities and the actual class labels. The most commonly used loss function for binary logistic regression is logistic loss(sometimes called cross-entropy loss). This function aims to minimize the log loss across all training observations. By penalizing incorrect predictions, the loss function encourages the model to produce probabilities that are closer to the true class labels.\n",
    "\n",
    "#### Optimization\n",
    "Gradient descent and its variants, like stochastic gradient descent (SGD), are common optimization techniques for logistic regression. Gradient descent works by computing the gradient (partial derivatives) of the loss function with respect to each parameter, and updating each parameter in the opposite direction of the gradient to minimize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619710c4",
   "metadata": {},
   "source": [
    "\n",
    "### Representation\n",
    "\n",
    "#### Logistic regression\n",
    "Logistic regression is common hypothesis class for classification\n",
    "\n",
    "$$ \\mathcal{X} = \\mathbb{R}^d \\quad \\mathcal{Y} = \\{1, -1\\} $$\n",
    "\n",
    "Now we use a linear predictor that outputs a continuous value in [0, 1]\n",
    "\n",
    "$$ h_w(\\mathbf{x}) = \\frac{1}{1 + e^{-\\langle \\mathbf{w}, \\mathbf{x} \\rangle}} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\mathbf{x} \\in \\mathcal{X}$ represents the input vector with dimension $d$\n",
    "* $\\mathbf{w}$ is the weight vector\n",
    "* $\\langle \\mathbf{w}, \\mathbf{x} \\rangle$ denotes the dot product between $\\mathbf{w}$ and $\\mathbf{x}$\n",
    "\n",
    "This linear predictor maps to:\n",
    "\n",
    "$$ h : \\mathcal{X} \\rightarrow [0, 1] $$\n",
    "\n",
    "#### One-versus-All Pseudo Code\n",
    "input:  \n",
    "* training set $S = (x_1, y_1), \\ldots, (x_m, y_m)$\n",
    "* algorithm for binary classification $ A $ (here $A$ is Logistic Regression)\n",
    "\n",
    "foreach $ i \\in \\mathcal{Y} $:   \n",
    "* let $ S_i = (x_1, (-1)^{\\mathbb{I}_{[y_1 \\neq i]}}), \\ldots, (x_m, (-1)^{\\mathbb{I}_{[y_m \\neq i]}}) $\n",
    "* let $ h_i = A(S_i) $\n",
    "\n",
    "output:  \n",
    "- the multiclass hypothesis defined by $ h(x) \\in \\arg\\max_{i \\in \\mathcal{Y}} h_i(x) $\n",
    "\n",
    "#### All-Pairs Pseudo Code\n",
    "input:  \n",
    "- training set $ S = (x_1, y_1), \\ldots, (x_m, y_m) $\n",
    "- algorithm for binary classification $ A $ (here $A$ is Logistic Regression)\n",
    "\n",
    "foreach $ i, j \\in \\mathcal{Y} $ such that $ i < j $:\n",
    "- initialize $ S_{i, j} $ to be the empty sequence\n",
    "- for $ t = 1, \\ldots, m $:\n",
    "  - If $ y_t = i $, add $ (x_t, 1) $ to $ S_{i, j} $\n",
    "  - If $ y_t = j $, add $ (x_t, -1) $ to $ S_{i, j} $\n",
    "- let $ h_{i, j} = A(S_{i, j}) $\n",
    "\n",
    "output:  \n",
    "- the multiclass hypothesis defined by  \n",
    "  $ h(x) \\in \\arg\\max_{i \\in \\mathcal{Y}} \\left( \\sum_{j \\in \\mathcal{Y}} \\text{sign}(j - i) h_{i, j}(x) \\right) $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef43d70c-38e7-47b7-a3ed-08c59dfa1103",
   "metadata": {},
   "source": [
    "\n",
    "### Loss\n",
    "\n",
    "For binary classification, logistic regression uses the sigmoid function:\n",
    "$$P(y = 1 | x) = \\sigma(w^{T}x + b)$$\n",
    "Where:\n",
    "* $x$ is the input vector\n",
    "* $w$ is the weights\n",
    "* $b$ is the bias\n",
    "* $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid function\n",
    "\n",
    "Binary Cross-Entropy Loss:\n",
    "$$L(y, \\hat y) = -(y log(\\hat y) + (1 - y)log(1 - \\hat y))$$\n",
    "Where:\n",
    "* $y$ is the true label (0 or 1)\n",
    "* $\\hat y$ is the predicted probability of the first class\n",
    "* and $\\hat y = \\sigma(w^T x + b)$\n",
    "\n",
    "One-vs-All:\n",
    "For one-vs-all, we have to train $K$ different classifiers for each class so that each classifier $k$ can learn to distinguish one class from all the others.\n",
    "The loss for the $i$-th example of classifier $k$ is:\n",
    "$$L_k(y^{(i)}, \\hat y_k^{(i)}) = -[y_k^{(i)}log(\\hat y_k^{(i)}) + (1 - y_k^{(i)})log(1 - \\hat y_k^{(i)})]$$\n",
    "Where:\n",
    "* $y_k^{(i)} = 1$ if the true class of the $i$-th example is class $k$, otherwise $y_k^{(i)} = 0$\n",
    "* $\\hat y_k^{(i)}$ is the predicted probability for class $k$\n",
    "\n",
    "The overall class is determined by selecting the classifier that has the highest probability (or confidence).\n",
    "\n",
    "All-Pairs:\n",
    "For All-Pairs, we have to train a classifier for every pair of classes instead of $K$ classifiers in One-vs-All training. For $K$ classes, we train $\\frac{K(K - 1)}{2}$ classifiers to distinguish between 2 classes for each classifier.\n",
    "\n",
    "The loss function is still the binary cross-entropy loss and rewritten for the $i$-th example as:\n",
    "$$L_{k,j}(y_{k,j}^{(i)}, \\hat y_{k, j}^{(i)}) = -[y_{k, j}^{(i)}log(\\hat y_{k, j}^{(i)}) + (1 - y_{k, j}^{(i)})log(1 - \\hat y_{k, j}^{(i)})]$$\n",
    "Each classifier will vote for one of two classes and the overall class is the class that receives the most votes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff72865c",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "### One-vs-All (OvR) with SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ff686",
   "metadata": {},
   "source": [
    "In One-vs-All, we train a separate binary classifier for each class. Each classifier learns to distinguish one class from all others. Below is the pesudo code on sample S."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a69fb",
   "metadata": {},
   "source": [
    "$\\text{Initialize parameters } \\mathbf{w} \\text{ for each class, learning rate } \\alpha, \\text{ and batch size } b$<br />\n",
    "$\\text{converge} = \\text{False}$<br />\n",
    "\n",
    "$\\text{while not converge:}$ <br />\n",
    "    $\\quad \\text{epoch} += 1$<br />\n",
    "    $\\quad \\text{Shuffle training examples}$<br />\n",
    "    $\\quad \\text{Calculate last epoch loss}$<br />\n",
    "    \n",
    "$\\quad \\text{for } i = 0, 1, \\dots, \\left\\lceil \\frac{n_{\\text{examples}}}{b} \\right\\rceil - 1 \\text{: } \\quad \\text{(iterate over batches)}$<br />\n",
    "        $\\quad \\quad X_{\\text{batch}} = X[i \\cdot b : (i + 1) \\cdot b] \\quad \\text{(select the } X \\text{ in the current batch)}$<br />\n",
    "        $\\quad \\quad \\mathbf{y}_{\\text{batch}} = \\mathbf{y}[i \\cdot b : (i + 1) \\cdot b] \\quad \\text{(select the labels in the current batch)}$<br />\n",
    "        $\\quad \\quad \\nabla L_{\\mathbf{w}} = \\mathbf{0} \\quad \\text{(initialize gradient matrix for each class)}$\n",
    "\n",
    "$\\quad \\quad \\text{for each pair of training data } (x, y) \\in (X_{\\text{batch}}, \\mathbf{y}_{\\text{batch}}) \\text{:}$<br />\n",
    "            $\\quad \\quad \\quad \\text{for } j = 0, 1, \\dots, n_{\\text{classes}} - 1 \\text{:}$<br />\n",
    "                $\\quad \\quad \\quad \\quad \\text{if } y = j \\text{:}$<br />\n",
    "                    $\\quad \\quad \\quad \\quad \\quad \\nabla L_{\\mathbf{w}_j} += \\left( \\sigma(\\mathbf{w}_j^T x) - 1\\right) \\cdot x  \\quad \\text{(for correct class)}$<br />\n",
    "                    $\\quad \\quad \\quad \\quad \\text{else:}$<br />\n",
    "                    $\\quad \\quad \\quad \\quad \\quad \\nabla L_{\\mathbf{w}_j} += \\sigma(\\mathbf{w}_j^T x) \\cdot x  \\quad \\text{(for other classes)}$<br />\n",
    "\n",
    "$\\quad \\quad \\text{for } j = 0, 1, \\dots, n_{\\text{classes}} - 1 \\text{:}$<br />\n",
    "            $\\quad \\quad \\quad \\mathbf{w}_j = \\mathbf{w}_j - \\alpha \\cdot \\frac{\\nabla L_{\\mathbf{w}_j}}{\\text{len}(X_{\\text{batch}})}  \\quad \\text{(update weights for each class)}$<br />\n",
    "\n",
    "$\\quad \\text{Calculate this epoch loss}$<br />\n",
    "    $\\quad \\text{if } \\left| \\text{Loss}(X, \\mathbf{y})_{\\text{this-epoch}} - \\text{Loss}(X, \\mathbf{y})_{\\text{last-epoch}} \\right| < \\text{CONV-THRESHOLD:}$<br />\n",
    "        $\\quad \\quad \\text{converge} = \\text{True}  \\quad \\text{(break the loop if loss converged)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b86973d",
   "metadata": {},
   "source": [
    "Here, $sigmoid(w_j^T x)$ gives the probability that \n",
    "ùë•\n",
    "x belongs to class \n",
    "ùëó\n",
    " (treated as a binary classification for that specific class)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d8e248",
   "metadata": {},
   "source": [
    "### All Pairs (OvO) with SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f650d12b",
   "metadata": {},
   "source": [
    "In All Pairs, we train a separate binary classifier for each pair of classes, focusing only on the data points belonging to the two classes in each pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafdcb87",
   "metadata": {},
   "source": [
    "$\\text{Initialize parameters } \\mathbf{w} \\text{ for each pair of classes, learning rate } \\alpha, \\text{ and batch size } b$<br />\n",
    "$\\text{converge} = \\text{False}$<br />\n",
    "\n",
    "$\\text{while not converge:}$<br />\n",
    "    $\\quad \\text{epoch} += 1$<br />\n",
    "    $\\quad \\text{Shuffle training examples}$<br />\n",
    "    $\\quad \\text{Calculate last epoch loss}$<br />\n",
    "    \n",
    "$\\quad \\text{for } i = 0, 1, \\dots, \\left\\lceil \\frac{n_{\\text{examples}}}{b} \\right\\rceil - 1 \\text{: } \\quad \\text{(iterate over batches)}$<br />\n",
    "        $\\quad \\quad X_{\\text{batch}} = X[i \\cdot b : (i + 1) \\cdot b] \\quad \\text{(select the } X \\text{ in the current batch)}$<br />\n",
    "        $\\quad \\quad \\mathbf{y}_{\\text{batch}} = \\mathbf{y}[i \\cdot b : (i + 1) \\cdot b] \\quad \\text{(select the labels in the current batch)}$<br />\n",
    "        $\\quad \\quad \\text{for each unique pair of classes } (A, B) \\text{:}$<br />\n",
    "            $\\quad \\quad \\quad \\nabla L_{\\mathbf{w}_{AB}} = \\mathbf{0} \\quad \\text{(initialize gradient for each pair (A, B))}$<br />\n",
    "            $\\quad \\quad \\quad \\text{for each } (x, y) \\in (X_{\\text{batch}}, \\mathbf{y}_{\\text{batch}}) \\text{:}$<br />\n",
    "                $\\quad \\quad \\quad \\quad \\text{if } y = A \\text{ or } y = B \\text{:} \\quad \\text{(focus on examples for classes A and B)}$<br />\n",
    "                    $\\quad \\quad \\quad \\quad \\quad \\text{if } y = A \\text{:}$<br />\n",
    "                        $\\quad \\quad \\quad \\quad \\quad \\quad \\nabla L_{\\mathbf{w}_{AB}} += \\left( \\sigma(\\mathbf{w}_{AB}^T x) - 1 \\right) \\cdot x  \\quad \\text{(for class A)}$<br />\n",
    "                    $\\quad \\quad \\quad \\quad \\text{else:}$<br />\n",
    "                        $\\quad \\quad \\quad \\quad \\quad \\quad \\nabla L_{\\mathbf{w}_{AB}} += \\sigma(\\mathbf{w}_{AB}^T x) \\cdot x  \\quad \\text{(for class B)}$<br />\n",
    "\n",
    "$\\quad \\quad \\quad \\mathbf{w}_{AB} = \\mathbf{w}_{AB} - \\alpha \\cdot \\frac{\\nabla L_{\\mathbf{w}_{AB}}}{\\text{len}(X_{\\text{batch}})}  \\quad \\text{(update weights for the pair (A, B))}$<br />\n",
    "\n",
    "$\\quad \\text{Calculate this epoch loss}$<br />\n",
    "    $\\quad \\text{if } \\left| \\text{Loss}(X, \\mathbf{y})_{\\text{this-epoch}} - \\text{Loss}(X, \\mathbf{y})_{\\text{last-epoch}} \\right| < \\text{CONV-THRESHOLD:}$<br />\n",
    "        $\\quad \\quad \\text{converge} = \\text{True}  \\quad \\text{(break the loop if loss converged)}$\n",
    "\n",
    "            \n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cee7a1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
