{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564d99a-8aff-424f-9e54-6b9018fec7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef43d70c-38e7-47b7-a3ed-08c59dfa1103",
   "metadata": {},
   "source": [
    "## **Overview of Multiclass Classification with Logistic Regression**\n",
    "\n",
    "### Algorithm Overview\n",
    "\n",
    "\n",
    "### Representation\n",
    "\n",
    "\n",
    "### Loss\n",
    "\n",
    "For binary classification, logistic regression uses the sigmoid function:\n",
    "$$P(y = 1 | x) = \\sigma(w^{T}x + b)$$\n",
    "Where:\n",
    "* $x$ is the input vector\n",
    "* $w$ is the weights\n",
    "* $b$ is the bias\n",
    "* $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid function\n",
    "\n",
    "Binary Cross-Entropy Loss:\n",
    "$$L(y, \\hat y) = -(y log(\\hat y) + (1 - y)log(1 - \\hat y))$$\n",
    "Where:\n",
    "* $y$ is the true label (0 or 1)\n",
    "* $\\hat y$ is the predicted probability of the first class\n",
    "* and $\\hat y = \\sigma(w^T x + b)$\n",
    "\n",
    "One-vs-All:\n",
    "For one-vs-all, we have to train $K$ different classifiers for each class so that each classifier $k$ can learn to distinguish one class from all the others.\n",
    "The loss for the $i$-th example of classifier $k$ is:\n",
    "$$L_k(y^{(i)}, \\hat y_k^{(i)}) = -[y_k^{(i)}log(\\hat y_k^{(i)}) + (1 - y_k^{(i)})log(1 - \\hat y_k^{(i)})]$$\n",
    "Where:\n",
    "* $y_k^{(i)} = 1$ if the true class of the $i$-th example is class $k$, otherwise $y_k^{(i)} = 0$\n",
    "* $\\hat y_k^{(i)}$ is the predicted probability for class $k$\n",
    "\n",
    "The overall class is determined by selecting the classifier that has the highest probability (or confidence).\n",
    "\n",
    "All-Pairs:\n",
    "For All-Pairs, we have to train a classifier for every pair of classes instead of $K$ classifiers in One-vs-All training. For $K$ classes, we train $\\frac{K(K - 1)}{2}$ classifiers to distinguish between 2 classes for each classifier.\n",
    "\n",
    "The loss function is still the binary cross-entropy loss and rewritten for the $i$-th example as:\n",
    "$$L_{k,j}(y_{k,j}^{(i)}, \\hat y_{k, j}^{(i)}) = -[y_{k, j}^{(i)}log(\\hat y_{k, j}^{(i)}) + (1 - y_{k, j}^{(i)})log(1 - \\hat y_{k, j}^{(i)})]$$\n",
    "Each classifier will vote for one of two classes and the overall class is the class that receives the most votes.\n",
    "\n",
    "\n",
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81faf1-f908-4641-acb6-fe2588587469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
